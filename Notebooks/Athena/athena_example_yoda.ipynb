{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d970fc16-0f1b-4245-9548-07b6f30289ca",
   "metadata": {},
   "source": [
    "From https://wiki.corp.ebay.com/display/COREAI/Athena+AI+Conference+2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733828e-4531-4dee-9c4d-be6096946a32",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "- Please use this docker image `hub.tess.io/gen-ai/ellement:latest` for your workspace.\n",
    "- Enable hadoop access to apollo-rno. Use your team's batch account.\n",
    "- If you \"pip installed\" packages in your workspace or have a conda environment, you might need to deactivate it.\n",
    "  - To backup your locally installed packages: `mv ~/.local ~/.local.bkp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13ca49-1c5d-459a-b4d5-d8c19f93ecf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating & Registering the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ae61fb0-4402-40bd-b9ba-d14ae3a261f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow, json\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2323fa99-84cb-425a-8417-4461cf6445b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file containing the training data. The file is in JSON format with keys: \"input\" and \"output\"\n",
    "# Example row: {input:\"I am happy.\", output:\"Happy, I am.\"}\n",
    "json_file = \"dataset.json\"\n",
    "\n",
    "# Please change to your directory\n",
    "# The HDFS folder in which we will store the training dataset for the Yoda project\n",
    "output_parquet_folder = \"/user/ppetrushkov/athena-aiconf2024/yoda/\"\n",
    "# The HDFS file name \n",
    "output_parquet_file = f\"{output_parquet_folder}train.hf.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8edcc08f-6cc6-4410-a1c5-fd279374d8f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Open the dataset file and load as Python object\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     js \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.json'"
     ]
    }
   ],
   "source": [
    "# Open the dataset file and load as Python object\n",
    "with open(json_file) as f:\n",
    "    js = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea114a-f6d8-40b7-a838-dc31720f195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset into a Pandas Dataframe  \n",
    "df_train = pd.DataFrame(js)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b48f0-052b-4080-b706-4e1f57147786",
   "metadata": {},
   "source": [
    "### Formatting the dataset to match training requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a493a3f-99e3-42cc-a06a-545462e717e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The structure of our dataset (input and output columns) \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Create messages in the OpenAI Chat format:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# One \"user\" turn with instruction and context; \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# One \"assistant\" turn that has the expected response\u001b[39;00m\n\u001b[1;32m      6\u001b[0m df_messages \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[ {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39minput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m      7\u001b[0m                                            {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: row\u001b[38;5;241m.\u001b[39moutput}\n\u001b[1;32m      8\u001b[0m                                          ] \n\u001b[0;32m----> 9\u001b[0m                                          \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_train\u001b[49m\u001b[38;5;241m.\u001b[39mitertuples()]\n\u001b[1;32m     10\u001b[0m                            })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "# The structure of our dataset (input and output columns) \n",
    "# Create messages in the OpenAI Chat format:\n",
    "# One \"user\" turn with instruction and context; \n",
    "# One \"assistant\" turn that has the expected response\n",
    "\n",
    "df_messages = pd.DataFrame({\"messages\": [[ {\"role\": \"user\", \"content\": f\"{row.input}\"}, \n",
    "                                           {\"role\": \"assistant\", \"content\": row.output}\n",
    "                                         ] \n",
    "                                         for row in df_train.itertuples()]\n",
    "                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17c33eeb-20b2-4475-8200-ec6d1b362218",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_messages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_messages\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_messages' is not defined"
     ]
    }
   ],
   "source": [
    "df_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fad046-d9bc-4ee1-b55a-d49b0f7dd08d",
   "metadata": {},
   "source": [
    "### Creating the Parquet file and uploading it to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b5d278-8e6d-4a81-9e2b-e63f51aa6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uuse pyarrow to transform the Pandas DataFrame into a Parquet file\n",
    "pdf = pyarrow.Table.from_pandas(df_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e7f6392-03f4-4977-bc07-887669bfe2be",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute 'hdfs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Connect to HDFS (with batch account set)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fs \u001b[38;5;241m=\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhdfs\u001b[49m\u001b[38;5;241m.\u001b[39mconnect()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute 'hdfs'"
     ]
    }
   ],
   "source": [
    "# Connect to HDFS (with batch account set)\n",
    "fs = pyarrow.hdfs.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18b6040-14fa-4bdf-ad47-1b58969a9640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 18:22:08,229 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n"
     ]
    }
   ],
   "source": [
    "# Write the file into the HDFS folder\n",
    "pq.write_table(pdf, f\"{output_parquet_file}\", filesystem=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82c98ddc-d118-4f33-844c-923020485aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 ppetrushkov hdmi-technology      16175 2024-06-03 18:23 /user/ppetrushkov/athena-aiconf2024/yoda/train.hf.parquet\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls {output_parquet_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb669a29-56c2-458d-8524-4554dc147278",
   "metadata": {},
   "source": [
    "### Registering the dataset with AIHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5eb8b0-6284-4b20-888a-64a244f522e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Option 1: go to [AIHub Datasets](https://aip.vip.ebay.com/data/data-set?projectName=athena-aiconf2024) to register the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc52cb4-5426-4401-b3dd-3895d875a262",
   "metadata": {},
   "source": [
    "#### Option 2: use Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6b3b4a5-869d-41c3-b70b-a8096b26bbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 18:49:14,208 - pykrylov.batch.dataset [MainThread  ] [ERROR]  Dataset creation failed. Error 403 {'detail': 'User is not allowed to update on given resource', 'instance': 'class com.ebay.taichi.trainingmanagement.service.DatasetService', 'status': 403, 'title': 'ForBidden', 'type': 'ClientError'}\n",
      "ERROR:pykrylov.batch.dataset:Dataset creation failed. Error 403 {'detail': 'User is not allowed to update on given resource', 'instance': 'class com.ebay.taichi.trainingmanagement.service.DatasetService', 'status': 403, 'title': 'ForBidden', 'type': 'ClientError'}\n"
     ]
    }
   ],
   "source": [
    "from pykrylov.batch.dataset import create_dataset, HdfsSpecific\n",
    "from pykrylov.batch.consts import DatasetFormat, DatasetSource\n",
    "\n",
    "# The project on AIHub. In our case \"athena-aiconf2024\" is a shared project where all Workshop participants are members.\n",
    "aip_project = \"athena-aiconf2024\"\n",
    "\n",
    "# Setting up the name with which the dataset will be registered on AIHub\n",
    "dataset_name = \"athena-yoda-v1.0\"\n",
    "\n",
    "# Register dataset\n",
    "hdfs_dataset = create_dataset(\n",
    "    name=dataset_name,\n",
    "    owner_domain=\"CoreAI\",\n",
    "    source=DatasetSource.HDFS,\n",
    "    data_format=DatasetFormat.PARQUET,\n",
    "    project=aip_project,\n",
    "    is_public=False,\n",
    "    hdfs_specific=HdfsSpecific(paths=[output_parquet_file]),\n",
    "    digest=None,\n",
    "    labeled_fields=[\"messages\"],\n",
    "    description=\"Yoda style finetuning using Athena\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9e68ecb8-2b37-49f1-a852-661852c36b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "    id: \"b26df8c6-206b-49b7-9164-f8a80e95484b\",\n",
       "    name: \"athena-yoda-style-output-v3.0\",\n",
       "    owner_domain: \"CoreAI\",\n",
       "    source: DatasetSource.HDFS,\n",
       "    format: DatasetFormat.PARQUET,\n",
       "    storage_managed: False,\n",
       "    project: \"athena-aiconf2024\",\n",
       "    is_public: False,\n",
       "    hdfs_specific: HdfsSpecific(\n",
       "        paths: ['/user/b_pynlp/athena-aiconf2024/yoda/train.hf.parquet'],\n",
       "    ),\n",
       "    labeled_fields: ['messages'],\n",
       "    description: \"Yoda style finetuning using Athena\",\n",
       "    created_by: \"ayouroukov\",\n",
       "    created_time: 1717150376254,\n",
       "    updated_by: \"ayouroukov\",\n",
       "    updated_time: 1717150376254,\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f715c-69ea-4c91-b432-c5a66502ab5a",
   "metadata": {},
   "source": [
    "# We're done! The dataset has been successfully registered and we're ready to begin fine tuning our base LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a248fd-97e6-4107-ae27-c5c012b5d0e1",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2368ac-3c38-472c-bba3-45370af374f4",
   "metadata": {},
   "source": [
    "# Next step: Finetuning the base model on our Yoda dataset using Athena \n",
    "### Please go to [AIHub Athena](https://aip.vip.ebay.com/data/athena?projectName=athena-aiconf2024) to begin the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55613ee-f706-4b2c-a5a9-d2e7118e2b36",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a1daa-9511-436c-b08f-d6ae7d71c9bb",
   "metadata": {},
   "source": [
    "Training has finished successfully?\n",
    "## Congratulations! You just fine tuned a LLM!  \n",
    "### Let's use Chomsky SDKs to interact with our newly trained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b0beb6-adfe-4505-beb6-970824d21aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip_project = \"ppetrushkov-project\"\n",
    "\n",
    "# Enter the adapater name you used for training\n",
    "adapter_name = \"athena-yoda\"\n",
    "\n",
    "# Enter the adapter version you used for training\n",
    "adapter_version = \"1\"\n",
    "\n",
    "# Base model used for training. Please see the adapter detail page for the exact name of the model\n",
    "base_model = \"ebay-internal-chat-completions-athena-lilium2-7b-chat\"\n",
    "\n",
    "# Let's construct the adapter full name/path. It's structure is <aip_project>/<adapter_name>/<adapter_version>\n",
    "adapter = f\"{aip_project}/{adapter_name}/{adapter_version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942d8a81-9c14-426e-a4fc-5cad287a31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pychomsky.chchat import EbayLLMChatWrapper\n",
    "from langchain.schema.messages import HumanMessage\n",
    "chat = EbayLLMChatWrapper(\n",
    "    model_name = base_model,\n",
    "    model_adapter = adapter,\n",
    "    max_tokens = 256,\n",
    "    temperature = 0.2,\n",
    "    top_p = 0.98,\n",
    "    presence_penalty = 0.0,\n",
    "    frequency_penalty = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b949637-08a0-499f-9eae-8dff6386e9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=' Athena is what? ' response_metadata={'model_name': 'ebay-internal-chat-completions-athena-lilium2-7b-chat', 'token_usage': {'completion_tokens': 7, 'prompt_tokens': 15, 'total_tokens': 22}} id='run-61a39b0d-5032-45e2-bac3-f095bd6142bd-0'\n"
     ]
    }
   ],
   "source": [
    "print(chat([HumanMessage(content='What is athena?')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4992c33-728e-4d67-be52-dca6ceeae9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
